{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Basics\n",
      "\n",
      "* **Types of statistics:**\n",
      "    * Descriptive statistics: procedures used to summarize, organize, and simplify data. It focuses on describing the given data. Hence, in computation it is to divide by $N$. \n",
      "    * Inferential statistics: procedures that allow for generalizations about population parameters based on sample statistics. It focuses on predicting/characterizing the features of population by analyzing the given data (i.e., a sample). In computation $N-1$ is divided by. \n",
      "* **Type of variables:** Nomial, Ordinal, Interval and Ratio\n",
      "    * Nomial: used to assign individual cases to categories\n",
      "    * Ordinal: used to rank order cases\n",
      "    * Interval: used to rank order cases, and the distance or interval between each value is equal\n",
      "    * Ratio: the same as interval variables but they have a \"true zero\"\n",
      "        * For example, Age (Age=0 literally means NO age)\n",
      "* **Z-score:** $Z=\\frac{x-\\mu}{\\sigma}$, where $\\mu$ is the mean value, and $\\sigma$ is the standard deviation. \n",
      "    * It is used to compare/communicate measures in different scales. For example, different temparature measures. \n",
      "    * **The Z-score is the standard scale in statistics, with mean $0$. **\n",
      "    * Raw scores can be converted to Z-scores.\n",
      "    * Z-scores can be used to find *percentile rank*\n",
      "        * raw scores $\\sim$ Z-scores $\\sim$ Percentile Rank\n",
      "\n",
      "* **Sampling error** is the difference between the population and the sample. Since we do not know the population, we use an estimation of the sampling error. \n",
      "\n",
      "  * Sampling error mainly depends on the size of the sample, relative to the size of the population. As sample size increases, sampling error decreases, under the. \n",
      "  * Sampling error also depends on the variance in the population. As variance increases, sampling error increases.\n",
      "  * **Standard error** (SE) is an estimate of amount of sampling error: $se=\\sigma/\\sqrt{N}$, where $N$ is the sample size, and $\\sigma$ is the standard deviation. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Measures"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Central Tendency**\n",
      "\n",
      "* **Mean** is the best measure of central tendency when the distribution is normal. \n",
      "* **Median** and **Mode** are preferred when there are extreme scores in the distribution, or positively or negatively skewed. \n",
      "\n",
      "**Variability.**\n",
      "\n",
      "* **Standard Deviation (SD)** is the average deviation from the mean in a distribution  \n",
      "* **Variance** = $\\mbox{SD}^2=\\frac{\\sum (x-\\mu)^2}{N}$\n",
      "* **Standard Error (SE)** = $SD/\\sqrt{N}$\n",
      "\n",
      "\n",
      "**Percentile Rank.** The percentile rank of a score is the percentage of scores in its frequency distribution that are the same or lower than it. That is, the percentage of scores that fall at or below a score in a distribution. \n",
      "    * If a normal distribution, it can be inferred fromt the standard score. If $Z=0$, then the percentage rank $=50^{th}$: 50% of the distribution falls below the mean. \n",
      "    * If not a normal distsribution, the mathematical formula is $\\frac{c+0.5 f}{N}\\times 100%$, where $c$ is the count of all scores less than the score of interest, $f$ is the frequency of the score of interest, and $N$ is the number of examinees in the sample. \n",
      "\n",
      "**Correlation** is a statistical procedure used to measure and describe the relationship between two variables. Correlation is often used to predict one variable's values based on the values of the other variable. Note that **correlation does not imply causaulity**. Several typical types of correlation measures are given: \n",
      "\n",
      "  * Pearson correlation coefficient ($r$) used when both variables are continuous; \n",
      "  * Point bi-serial correlation used when one variable is continuous and the other is dichotomous;\n",
      "  * Phi coefficient: when both variables are dichotomous;\n",
      "  * Spearman rank correlation: when both variables are ordinal (ranked data).\n",
      "\n",
      "**Reliability of Measurements.** According to the classical test theory, the raw scores of a variable (measurements) are not perfect and often influenced by bias and chance error. In a perfect world, it would be possible to obtaine a \"true score\" rather than a \"raw score\": $\\mathcal{X}=\\mbox{true score}+bias+error$, which is known as \"true score theory\". A measure is considered to be reliable as it approaches the true score. Three method are used to estimate the reliability:\n",
      "\n",
      "   1. Test/re-test: take two measures and then the correlation between $\\mathcal{X}_1$ and $\\mathcal{X}_2$ is an estimate of the reliability. => time-consuming\n",
      "   2. Parallel tests: use two different ways to obtain two measures, then the correlation between them is an estimate of the reliability. => cost expensive\n",
      "   3. Inter-item estimates: it is the most commonly used method in the social science. For example, suppose a 20-item survey is designed to measure extraversion. We can randomly select 10 items to get subset $A$ ($\\mathcal{X}_1$), and the other 10 items become subset $B$ ($\\mathcal{X}_2$). Then, the correlation between $\\mathcal{X}_1$ and $\\mathcal{X}_2$ is an estimate of reliability. \n",
      "   \n",
      "**Validity of a Construct.** A **construct** is an ideal \"object\" that is not directly observable, as opposed to \"real\" observable objects. For example, \"intelligence\" is a construct. The process of defining a construct to make it observation and quantifiable is called \"operationlize a construct\", such as intelligence test. The construct validity includes: \n",
      "\n",
      "  * Content validity: does the test contain proper content?\n",
      "  * Convergent validity: does the test correlate with other, established measures of verbal ability?\n",
      "  * Divergent validity: does the test correlate less well with measures designed to test a different type of ability?\n",
      "  * Nomological validity: are scores on the test consistent with more general theories?\n",
      " "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Multiple Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### General Linear Model (GLM)\n",
      "\n",
      "GLM is the mathematical framework used in many common statistical analysis, including multiple regression and ANOVA. ANOVA is typically presented as distinct from multiple regression, but it is a multiple regression. \n",
      "\n",
      "$$\\hat{Y}=B_0+\\sum_i B_i X_i$$\n",
      "\n",
      "**Categorical Variables** can be incorporated by dummy codes. **Dummy coding** is a system to code categorical predictors in a regression analysis. For example, \n",
      "\n",
      "* IV (independent variables): area of research in a Psychology department\n",
      "    * Cognitive, encoded to $D_1 D_2 D_3=000$ (hence, the interceptor represents the coefficient) or $D_1=-1, D_2=-1, D_3=-1$, all the codes $D_1, D_2, D_3$ are used as independent variables for regression analysis. \n",
      "    * Clinical, encoded to 100\n",
      "    * Developmental, encoded to 010\n",
      "    * Social, encoded to 001\n",
      "* DV (dependent variables): number of publications\n",
      "\n",
      "* In the coding, using \"Y ~ X1+X2+C(category)\"\n",
      "\n",
      "### Moderation\n",
      "\n",
      "**A moderator variable ($Z$) will enhance a regression model if the relationship between $X$ and $Y$ varies as a function of $Z$**. That is, the correlation between $X$ and $Y$ is NOT consistent across the distribution of $Z$, i.e., the effect of $Z$ really matters. For example,  for a regression model: $Y=B_0+B_1 X+e$, if there is a moderator variable $Z$, then $B_1$ will NOT be representative across all $Z$. In other words, **the relationship between $X$ and $Y$ is different at different levels of $Z$**. \n",
      "\n",
      "**For continuous variables:**  \n",
      "\n",
      "* The regression model: $Y=B_0+B_1 X+B_2 Z+e$  \n",
      "* The **moderated** regression model: $Y=B_0+B_1 X+B_2 Z +B_3 (XZ)+e$  \n",
      "\n",
      "**For categorical variables:**  \n",
      "\n",
      "* The regression model: $Y=B_0+B_1 D_1+B_2 D_2+B_3 Z+e$, where $D_1$ and $D_2$ are dummy codes.   \n",
      "* The **moderated** regression model: $Y=B_0+B_1 D_1+B_2 D_2+B_3 Z+B_4(D_1Z)+B_5(D_2Z)+e$ \n",
      "\n",
      "**Evaluation:**\n",
      "\n",
      "* Using an ANOVA test to compare the regression model and the moderated regression model in terms of variance explained ($R^2$)\n",
      "* Evaluate the coefficients of predictors associated with the moderation effect, i.e., $(XZ)$, $(D_1Z)$ and $(D_2Z)$\n",
      "\n",
      "**Centering predictors:** $X_c=X-M$, where $M$ is the mean of values of variable $X$. In the regression model, we use centering variable $X_c$ instead of variable $X$. The main motivation is to interpret the intercept $B_0$ easier and more meaningful: $B_0$ is the mean prediction value of $Y$ when all predictors are $0$ after centering. Without centering, there is no clear (and intuitive) meaning for $B_0$. On the other hand, centering can help avoid *multicolinearity*. In conclusion, the $B_0$ represents the mean of predicted $Y$ values, and the coefficients of other stable variables remain the same. \n",
      "\n",
      "* **Multicolinearity:** when two predictor variables $X, Z$ in a GLM are so highly correlated that they are essentially redundant and it becomes difficult to estimate coefficient values associated with each predictor, i.e., their product (the coefficient of $XZ$) is more stable (remaining the same), and the variance explained ($R^2$) will not be changed either. \n",
      "\n",
      "**After centering, for continuous variables:**  \n",
      "\n",
      "* The regression model: $Y=B_0+B_1 X_c+B_2 Z_c+e$  \n",
      "* The **moderated** regression model: $Y=B_0+B_1 X_c+B_2 Z_c +B_3 (X_cZ_c)+e$  \n",
      "\n",
      "**After centering, for categorical variables:**  \n",
      "\n",
      "* The regression model: $Y=B_0+B_1 D_1+B_2 D_2+B_3 Z_c+e$, where $D_1$ and $D_2$ are dummy codes.   \n",
      "* The **moderated** regression model: $Y=B_0+B_1 D_1+B_2 D_2+B_3 Z_c+B_4(D_1Z_c)+B_5(D_2Z_c)+e$ \n",
      "\n",
      "### Mediation\n",
      "\n",
      "A mediation analysis is typically conducted to better understand an observed effect of an independent variable on a dependent variable, or a correlation between $X$ and $Y$. **A mediator variable ($M$) accounts for some (i.e., partial mediation) or all (i.e., full mediation) of the relationship between $X$ and $Y$.** Intuitively, for the correlation ($X\\rightarrow Y$), after adding a meditor varialbe, it becomes $(X\\rightarrow M \\rightarrow Y)$. That is, **$X$ influences $Y$ BECAUSE of $M$**. Note that **there is a BIG difference between statistical mediation and true causal mediation. **\n",
      "\n",
      "In practice, we can run three regression models to test the statistical mediation: \n",
      "\n",
      "* $lm(Y\\sim X)$: regression coefficient for $X$ should be significant.\n",
      "* $lm(M\\sim X)$: regression coefficient for $X$ should be significant.\n",
      "* $lm(Y\\sim X+M)$: regression coefficient for $M$ should be significant, whereas the **regression coefficient for $X$ is no longer significant**. \n",
      "\n",
      "For **full mediation**, it means: \n",
      "\n",
      "* The direct effect is no longer significant after adding the mediator variable into the regression. (If it is still significant, then we may say partial mediation)\n",
      "* The Sobel test is significant. \n",
      "\n",
      "**Path Analysis** is another approach to conduct mediation analysis other than the above mentioned *standard approach*. Mediation analyses are typically **illustrated** using \"path models\". It is also possible to test for mediation using a statistical procedure called *Structural Equation Modeling* (SEM). \n",
      "\n",
      "* Rectangels: observed variables ($X, Y, M$)\n",
      "* Circles: unobserved variables ($e$)\n",
      "* Triangles: constants\n",
      "* Arrows: associations \n",
      "\n",
      "Path model for $Y=B_0+B_1X+e$ is illustrated by: \n",
      "\n",
      "<img src=\"files\\pics\\pm1.png\"/>\n",
      "\n",
      "Path model with a mediator is given by: \n",
      "\n",
      "<img src=\"files\\pics\\pm2.png\"/>\n",
      "\n",
      "To avoid confusion, let's label the paths: \n",
      "\n",
      "* $a$: path from $X$ to $M$\n",
      "* $b$: path from $M$ to $Y$\n",
      "* $c$: path from $X$ to $Y$ (before including $M$)\n",
      "* $c'$: path from $X$ to $Y$ (after including $M$)\n",
      "* Note: $(a*b)$ is known as the indirect path\n",
      "\n",
      "**Sobel test** is testing the effect of indirect path, that is, whether the indirect path from $X$ through $M$ to $Y$ is statistically significant. \n",
      "\n",
      "* The sobel test\n",
      "\n",
      "> $$z=\\frac{B_a*B_b}{\\sqrt{B_a^2*SE_b^2+B_b^2*SE_a^2}}$$\n",
      "\n",
      "* The `null` hypothesis: the indirect effect is zero, i.e., ($B_a*B_b)=0$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Significance Test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Null Hypothesis Significance Test (NHST)\n",
      "\n",
      "* $H_0=$ `null` hypothesis, $H_A=$ `alternative` hypothesis\n",
      "* **p value:** assume $H_0$ is true, then calculate the probability of observing data with these characteristics, given that $H_0$ is true. \n",
      "  * $p=P(D|H_0)$: if $p$ is very low, then reject $H_0$, else retain $H_0$.\n",
      "  * NOT: the probability of the null hypothesis being true is $p$.\n",
      "  * In other words, $P(D|H_0)!=P(H_0)$\n",
      "* **t value:** $t=B/SE$, where $B$ is the unstandardized regression coefficient, $SE$ is the standard error, $SE=SQRT(SS.Resiqual/(N-2))$\n",
      "  * Note if $N$ is large, then $t$ tends to be high and $p$ is likely to be small and the null hypothesis is rejected. In other words, if we have a large sample, no matter what we test, it tends to be *statistically significant*. \n",
      "  * The $t$ distribution is the normal distribution, according to the central limit theorem. \n",
      "  * $p$-value is a function of $t$ and sample size. \n",
      "* Arbitary decision rule\n",
      "  * The cut-off value (alpha) is arbitary\n",
      "  * $p< 0.05$ is considered standard but still arbitary\n",
      "  * Problems arise when $p$ is close to 0.05 but not less than 0.05\n",
      "  \n",
      "Truth\\experiments | Retain $H_0$ | Reject $H_0$ \n",
      "--- | --- | --- \n",
      "$H_0$ true | Correct Decision | Type I error (False alarm) \n",
      "$H_0$ false | Type II error (Miss) | Correct Decision\n",
      "  \n",
      "* **Confidence intervals:** an interval estimate of a population parameter, based on a random sample. \n",
      "  * Confidence intervals for mean: usually the mean is a single estimate of a sample, the logic of conifdence intervals is to report a range of values (i.e., an interval), rather than a single value. \n",
      "  * Confidence intervals for regression coefficient: similarly, the coefficient is usually a single estimate of a sample, the confidence intervals are to report a range of estimates, rather than a single value. \n",
      "  * The main argument for interval estimates is the reality of a sampling error. Sampling error implies that point estimates will vary from one study to the next. \n",
      "  * A researcher will therefore be more confident about accuracy with an interval estimate. \n",
      "\n",
      "#### Sampling Distribution\n",
      "\n",
      "It is a distribution of sample statistics, obtained from **multiple samples**, each of size $N$. In particular, it could be:\n",
      "\n",
      "* distribution of sample means\n",
      "* distribution of sample correlations\n",
      "* distribution of sample regression coefficients\n",
      "\n",
      "#### Central Limit Theorem\n",
      "\n",
      "Three principles: \n",
      "\n",
      "* The mean of a sampling distribution is the same as the mean of the population.\n",
      "* The standard deviation of the sampling distribution is the square root of the variance of sampling distribution: $\\sigma^2=\\sigma^2/N$\n",
      "* The shape of a sampling distribution is approximately normal if either (a) $N\\ge 30$; or (b) the shape of the population distribution is normal. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Student's t-test\n",
      "\n",
      "Student's t-test is developed by William Gossett in 1908 who use the pen name \"Student\" to publish his work. Hence, it is called **Student's t-test**.  \n",
      "\n",
      "Test | Observed | Expected | SE | df\n",
      "--- | --- | --- | --- | ---\n",
      "z | Sample mean | Population mean | SE of the mean | NA\n",
      "t (single sample) | Sample mean | Population mean | SE of the mean | N-1\n",
      "t (dependent) | Sample mean of difference scores | Population mean of difference scores | SE of the mean difference | N-1\n",
      "t (independent) | Difference between two sample means | Difference between two population means | SE of the difference between means | (N1-1)+(N2-1)\n",
      "\n",
      "**Z-test** is used when comparing a sample mean to a population mean, and the standard deviation of the population is known. \n",
      "\n",
      "$$z=\\frac{\\mbox{Observed - Expected}}{SE}$$\n",
      "\n",
      "**Single sample t-test** is used when comparing a sample mean to a population mean, and the standard deviation of the population is **NOT** known.\n",
      "\n",
      "$$t=\\frac{\\mbox{Observed - Expected}}{SE}$$\n",
      "\n",
      "**Dependent t-test**, a.k.a **paired samples t-test**, is used when evaluating the difference between **two related samples**. It is appropriate when the same objects are being compared, e.g., pre/post design; or when two samples are matched at the level of individual subjects, e.g., allowing for a difference score to be calculated. Note that the difference score should be meaningful. \n",
      "\n",
      "> `t.test(australia.red, australia.white, paired=T)`  \n",
      "> `cohensD(australia.red, australia.white, method='paired')` \n",
      "\n",
      "where Cohen'd $d$ represents the *effect size:* Cohen's $d=M/SD$\n",
      "\n",
      "**Independent t-test** is used when evaluating the difference between **two independent samples**, e.g., males and females, patients and healthy controls, for samples derived from different groups (of subjects or conditions). \n",
      "\n",
      "$$\\begin{align}\n",
      "t=\\frac{M1-M2}{SE}, & SE  = \\frac{SE1+SE2}{2} \\\\\n",
      "\\mbox{Cohen's } d=\\frac{M1-M2}{SD_{\\mbox{pool}}}, & SD_{\\mbox{pool}}=\\frac{SD1+SD2}{2} \n",
      "\\end{align}$$\n",
      "\n",
      "> `t.test(Days8, Days12, var.equal=T)`  \n",
      "> `cohensD(Days8, Days12, method='pooled')`\n",
      "\n",
      "Homogeneity of variance assumption for independent t-test, i.e., to verify `var.equal=T`: \n",
      "\n",
      "* The pooled SD is appropriate only if the variances in the two groups are equivalent; otherwise the homogeneity of variance assumption is violated. \n",
      "* How to detect a violation: conduct Levene's test which is a significance test for the difference of variances rather than means. If significant then the homogeneity of variance assumption is violated. \n",
      "\n",
      "> `levene.test(data$IQ ~ data$condition)`\n",
      "\n",
      "* What to do if violated? (1) adjust df and p-value (Welch's procedure); (2) use a non-parametric test. \n",
      "\n",
      "Confidence interval: \n",
      "\n",
      "* Upper bound = M + t (SE)\n",
      "* Lower bound = M - t (SE)\n",
      "* t-value depends on levels of confidence and t-distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### ANOVA: Analysis of Variance\n",
      "\n",
      "Student's t-test works well for one or two samples, but what if we have multiple samples? Doing multiple student's t-tests are tedious, and increasing Type I error (i.e., false-alarm error). **The ANOVA test is more suited in the case where there are multiple samples. It is used to compare means via analysis of variance**. Formally, it is appropriate when the predictors (IVs) are all categorical and the outcome (DV) is continuous. If the samples are independent, it is called **between groups ANOVA**; otherwise **repeated measures ANOVA**. \n",
      "\n",
      "#### One-way ANOVA\n",
      "\n",
      "ANOVA typically involves NHST with `null` hypothesis: all groups are equal. The test statistic is the F-test (F-ratio). \n",
      "\n",
      "* F = (Variance between groups' means)/(Variance within groups) = $\\mbox{MS}_A / \\mbox{MS}_{S/A}$\n",
      "\n",
      "> `anova.WMT <- aov(data$IQ ~ data$condition)`\n",
      "\n",
      "The F-test has a family of F-distributions which depends on (1) number of subjects per group; and (2) number of groups. \n",
      "\n",
      "Source | SS | df | MS | F\n",
      "--- | --- | --- | --- | ---\n",
      "A | $n\\sum(Y_j-Y_T)^2$ | a-1 | $\\mbox{SS}_A$/$\\mbox{df}_A$ | $\\mbox{MS}_A$/$\\mbox{MS}_{S/A}$\n",
      "S/A | $\\sum (Y_{ij}-Y_j)^2$ | a(n-1) | $\\mbox{SS}_{S/A}$/$\\mbox{df}_{S/A}$ | ----\n",
      "Total | $\\sum (Y_{ij}-Y_T)^2$| N-1 | ---- | ----\n",
      "\n",
      "where $Y_T$ is the ground mean, $Y_j$ is the group mean, and $Y_{ij}$ is the individual score. \n",
      "\n",
      "Homogeneity assumption for one-way ANOVA: \n",
      "\n",
      "* Within-group variance is equivalent for all groups, can be verified by Levene's test. \n",
      "\n",
      "**Post-hoc tests**, such as Tukey\u2019s procedure, allow for **multiple pairwise comparisons without an increase in the probability of a Type I error**, that is, this procedures can preserve the same results as well as derived by individual Student's t-test. There are many procedures for post-hoc tests, and the degree to which p-values are adjusted varies according to the procedure. \n",
      "\n",
      "> `TukeyHSD(anova.WMT)`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Factorial ANOVA = ANOVA with 2+ IVs and 1 DV\n",
      "\n",
      "Factorial ANOVA is often denoted, e.g., $2\\times 3\\times 4$ ANOVA = an ANOVA with 3 IVs, one of which has 2 levels, one of which has 3 levels, and the last of which has 4 levels.  To calculate a factorial ANOVA, we first have to divide our data into **cells**. For example, if we have a $2\\times 3$ ANOVA, two IVs are `Gender` and `Age`. We have 2x3 = 6 cells.\n",
      "\n",
      " | Young | Medium | Old | Marginal\n",
      " --- | --- | --- | --- | ---\n",
      " **Male** | Cell #1 mean | Cell #2 mean | Cell #3 mean | (mean1+mean2+mean3)/3\n",
      " **Femal** | Cell #4 mean | Cell #5 mean | Cell #6 mean | (mean4+mean5+mean6)/3\n",
      " **Marginal** | (mean1+mean4)/2 | (mean2+mean5)/2 | (mean3+mean6)/2 | \n",
      "  \n",
      " Cell means differ ($SS_{error}$) for 4 reasons: \n",
      " \n",
      " 1. Error\n",
      " 2. Effects of IV #1 (Gender): $SS_{IV1}$\n",
      " 3. Effeccts of IV #2 (Age): $SS_{IV2}$\n",
      " 4. Effects of interactions: $SS_{IV1*IV2}$\n",
      "\n",
      "#### Two-way ANOVA\n",
      "\n",
      "It has **two independent variables (IVs) and one dependent variable (DV)**. If A has 2 levels and B has two levels (hence can be denoted as 2x2 ANOVA), then A\\*B has four different combinations. The two-way ANOVA is to investigate the statistical significance of the difference between any two combinations. Note that there is also three-way ANOVA which is also factorial AVONA, but here we only consider two-way ANOVA. \n",
      "\n",
      "> `anova <- aov(dat$errors ~ dat$driving * dat$conversion)`  \n",
      "> `summary(anova)`\n",
      "\n",
      "The effects of factorial ANOVA include: \n",
      "\n",
      "* **Main effect:** the effect of one IV averaged across the levels of the other IV. \n",
      "\n",
      "> Post-hoc tests\n",
      "\n",
      "* **Interaction effect:** the effect of one IV depends on the other IV. A significannnt interaction between two IVs means that one IV's value changes as a function of the other, but gives no specific information. The most simple and common method of interpreting interactions is to look at a graph. \n",
      "\n",
      "> Analysis of simple effects by conducting a serires of one-way ANOVA (or t-tests)\n",
      "\n",
      "* **Simple main effect:** the effect of one IV at a particular level of the other IV. \n",
      "\n",
      "> t-test\n",
      "\n",
      "Note that main effect and interaction effect are independent from one another. Factorial ANOVA is just a special case of multiple regression with **perfectly independent predictors**: \n",
      "\n",
      "$$Y=B_0+B_1X_1+B_2X_2+B_3X_3 +e$$\n",
      "\n",
      "where $X_1=A$ and $X_2=B$ are two IVs, $Y$ is the DV, and **$X_3 =(A*B)$**.\n",
      "\n",
      "The underlying assumptions of factorial ANOVA: \n",
      "\n",
      "* DV is continuous (interval or ratio variable)\n",
      "* DV is normally distributed\n",
      "* Homogeneity of variance: `levene.test(dat$errors ~ dat$driving * dat$conversion)`\n",
      "\n",
      "**Effect size** in factorial ANOVA: \n",
      "\n",
      "* complete $\\eta^2=SS_{effect}/SS_{total}$\n",
      "* partial $\\eta^2 = SS_{effect}/(SS_{effect}+SS_{S/AB})$, this partial value is usually adopted. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Repeated Measures\n",
      "\n",
      "Repeated measures are appropriate when comparing group means\n",
      "\n",
      "* Three or more group means;\n",
      "* Same subjects tested in each condition; \n",
      "\n",
      "\n",
      " | Pros | Cons \n",
      "--- | --- | ---\n",
      "1 |Less cost (fewer subjects required);  | Order effects \n",
      "2 | **More statistical power**;  | Counterbalancing\n",
      "  | 2.l Variance across subjects may be systematic  | \n",
      "  | 2.2 If so, it will not contribute to the error term | \n",
      "3 | | Missing data\n",
      "4 | | Extra assumption\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Chi-square Tests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note.** Different from the previous tests, **Chi-square tests are used when outcome and predictor variables are all categorical**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Chi-square goodness of fit\n",
      "\n",
      "**Objective:** determins how well a distribuiton of proportions \"fit\" an expected distribution. For example, in election polls, is there a statistically significant difference in voter preference among candidates?\n",
      "\n",
      "#### Chi-square goodness of fit is an NHST\n",
      "\n",
      "**`Null` hypothesis:** equal proportions, **`alternative` hypothesis:** unequal proportions. \n",
      "\n",
      "$$\\begin{align}\n",
      "\\chi^2 &=\\sum [(O-E)^2/E] \\\\\n",
      "O & = Observed \\\\\n",
      "E & = Expected \\\\\n",
      "df & = \\mbox{# of categories} -1 \n",
      "\\end{align}$$\n",
      "\n",
      "$p$-value depends on $\\chi^2$ and `df`. \n",
      "\n",
      "#### Effect Size\n",
      "\n",
      "To estimate effect size: Cramer's $V$: \n",
      "\n",
      "$$\\Phi_c = \\sqrt{\\chi^2/N(k-1)}$$\n",
      "\n",
      "where $N$ is the sample size, and $k$ is the number of categories. \n",
      "\n",
      "#### Chi-square goodness of fit in R\n",
      "\n",
      "> Observed <- table(Election$Candidate)  \n",
      "> chisq.test(Observed)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Chi-square test of independence\n",
      "\n",
      "**Objective: determines whether there is a relationship between two categorical variables**. \n",
      "\n",
      "For example, in election polls, is there a relationship between voter gender and preference among candidates? Suppose that for New York city mayoral election, \n",
      "\n",
      "* assume a small poll was conducted (N=200)\n",
      "* there are more males than females (n=140, n=60)\n",
      "* do you intend to vote for: Christine Quinn, Joseph Lhota, or other?\n",
      "\n",
      "The data we collect is: \n",
      "\n",
      " | Quinn | Lhota | Other\n",
      "--- | --- | --- | ---\n",
      "**Female** | 40 |10 | 10\n",
      "**Male** | 90 | 40 | 10\n",
      "\n",
      "Since there are more males than females in essence, it is not clear for us whether there is a relationship between voter gender and preference among candidates?\n",
      "\n",
      "**`Null` hypothesis:** there is no relationship between ..., **`alternative` hypothesis:** there is a relationship between ...\n",
      "\n",
      "**Note that the formula for Chi-square test of independence is the same as the Chi-square test for good of fit. **\n",
      "\n",
      "* **Compute the expected frequency:** the proportion of male and female voters for each candidate should be the same as the overall voter preference rates. \n",
      "* $E=(R/N)* C $, where $E$ is the expected frequency, $R$ is the number of entries in the cell's row, $N$ is the total number of entries, and $C$ is the number of entries in the cell's column.  \n",
      "\n",
      "\n",
      " | Quinn | Lhota | Other\n",
      "--- | --- | --- | ---\n",
      "**Female** | (60/200)*130=39 | (60/200)*50=15 | (60/200)*20=6\n",
      "**Male** | (140/200)*130=91 | (140/200)*50=35 | (140/200)*20=14\n",
      "\n",
      "After getting the **expected** data, by comparing with the **observed** data, we can run the Chi-square goodness of fit test. The results are: $\\chi^2 = 6.23, df=2, p=0.04$, resulting in the rejection of the `null` hypothesis. The conclusion is that more man choose Lhota and Other than expected. \n",
      "\n",
      "> Observed <- table(Election\\$Candidate, Election\\$Gender)  \n",
      "> chisq.test(Observed)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Binary Logistic Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note.** It is appropriate when predicting **a binary categorical outcome variable** from a set of predicto variables that may be **continuous and/or categorical**. More specifically, if outcome has two levels (0 and 1), we use binary logistic regression; if outcome has multiple levels, we can use multimomial regression. \n",
      "\n",
      "**Formula:** $\\mbox{logit} = \\ln (\\hat{Y}/(1-\\hat{Y})) = B_0 +\\sum (B_k X_k)$, where $\\hat{Y}=P(\\mbox{outcome})$\n",
      "\n",
      "**Evaluation:** to test the overall model, we can \n",
      "\n",
      "* Model chi-square: compare the chi-square for the model to the chi-square of a model with no predictor (the `null` model)\n",
      "\n",
      "> with(lrfit, null.deviance - deviance) # difference in deviance for the two models  \n",
      "> with(lrfit, df.null - df.residual) # df for the difference between the two models  \n",
      "> with(lrfit, pchisq(null.deviance-deviance, df.null-df.residual, lower.tail=FALSE)) # p-value\n",
      "\n",
      "* **Wald test:** compare multiple models\n",
      "\n",
      "> library(aod)  \n",
      "> wald.test(b=coef(lrfit), Sigma=vcov(lrfit), Terms=2) #danger   \n",
      "> wald.test(b=coef(lrfit), Sigma=vcov(lrfit), Terms=3) #rehab\n",
      "\n",
      "* Classification success:  does the model classify cases correctly?\n",
      "\n",
      "> glm(formula = Y ~ X1 + X2 + X3, family = binomial)\n",
      "\n",
      "> The difference between `lm` and `glm` is that the latter is the generalized linear model (and can be used for binary logistic regression, for example) whereas the former is the general linear model. \n",
      "\n",
      "> exp(coef(Y)) # exponentiated coefficients\n",
      "\n",
      "> ClassLog(Y, X1): percentage of success using X1\n",
      "\n",
      "**Remark.** When there are more than 2 categories on the outcome, we can run **multinomial logistic regression**. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Revisit the Assumptions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far, we have two main  primary constraints of the assumptions: \n",
      "\n",
      "* **Normal distribution in Y (DV)**, which can be tested by \n",
      "    * Histograms and summary statistics: skew; kurtosis; outliers\n",
      "        * Rule of thumb is (**skew>3**) and/or (**kurtosis>10**) indicates a non-normal distribution\n",
      "    * Q-Q plot\n",
      "        * It is a plot of the sorted values from the data set against the expected values of the corresponding quantiles from the standard normal distribution. \n",
      "        * If the distribution is normal, then the plotted points should approximately lie on a straight line. \n",
      "    * Empirical tests, such as D'Agostino's $K^2$ test\n",
      "* **Linear relationship between predictor variables and outcome variables. **\n",
      "\n",
      "**Transformation. ** If a distribution is not normal, then it is sometimes possible to transform the data in an attempt to make the distribution normal. A **transformation** is a single function applied to all data points in the distribution (for example, square root), hence the values are changed, but the rank order of cases (i.e., all data points) should remain the same.  \n",
      "\n",
      "* The most common transformations for **positive skew are: square root, logarithm, inverse**; \n",
      "* The most common transformations for **negative skew are: reflect and square root, reflect and logarithm, reflect and inverse**; "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Non-Parametric Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Parametric vs. Non-parametric\n",
      "\n",
      "* Parametric statistics are used to make inferences about **population parameters**. The inferences about population parameters are not valid if **all assumptions** are not met. In this case, a \"quick fix\" (e.g., transformation) is possible and the parametric approach is still considered to be valid. \n",
      "\n",
      "* **Non-parametric statistics do not assume that the data or population have any characteristic structure**. \n",
      "\n",
      "* All the studied tests also contain a version of `non-parametric`\n",
      "    * Correlation\n",
      "        * Spearman's rank correlation coefficient\n",
      "        * Kendall's tau\n",
      "    * Regression\n",
      "        * Non-parametric regression\n",
      "    * **t-tests**\n",
      "        * Independent: Mann-Whitney U test\n",
      "        * Dependent: Wilcoxan signed-rank test\n",
      "    * ANOVA\n",
      "        * Kruskal-Wallis one-way\n",
      "        * Friedman two-way\n",
      "    * Chi-square\n",
      "        * McNemar's test\n",
      "        * Fisher's exact test\n",
      "        \n",
      "#### Non-parametric Paired Samples t-test: Wilcoxan's signed-rank method\n",
      "\n",
      "`Null` hypothesis: the **median** difference between pairs = 0; and `Alternative` hypothesis: the median difference between pairs !=0. \n",
      "\n",
      " | Red | White | Sign(S) | ABS | Rank(R) | S\\*R   \n",
      "---|---|---|---|---|---|---\n",
      " 1 | 65 | 60 | -1 | 5 | 1 | -1\n",
      " 2 | 60 | 70 | +1 | 10 | 2 | 2 \n",
      " 3 | 65 | 80 | +1 | 15 | 3 | 3\n",
      " 4 | 85 | 65 | -1 | 20 | 4 | -4\n",
      " \n",
      " \n",
      " Since $V=\\sum S*R=0$, the `null` hypothesis is True. If $V\\gg 0$, the `null` hypothesis is false and should be rejected. \n",
      " \n",
      " > wilcox.test(france.red, france.white, paired=TRUE)\n",
      " \n",
      "#### Non-parametric Independent t-test: Mann & Whitney U test\n",
      "\n",
      "* Adapted Wilcoxan's ranking method to compare independent groups\n",
      "* Arrange all observations, regardless of group, into a single ranked series. \n",
      "* The sum of all ranks, $\\mbox{Sum}=N(N+1)/2$\n",
      "    * R = Sum of Ranks from one group\n",
      "    * U = Sum - R\n",
      "    \n",
      "   \n",
      " | Rating | Group | Rank(R)\n",
      " ---|---|---|---\n",
      " 1 | 60 | A | 1\n",
      " 2 | 70 | B | 2\n",
      " 3 | 80 | B | 3\n",
      " 4 | 90 | A | 4\n",
      "\n",
      "Hence, R=1+4=5, U=Sum(1+2+3+4)-R=5, i.e., groups A and B are independent. \n",
      "\n",
      "> wilcox.test(australia.white, frace.white, paired=FALSE)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}